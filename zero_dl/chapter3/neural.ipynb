{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488bdf2d",
   "metadata": {},
   "source": [
    ">**ニューラルネットワーク**は先ほどのパーセプトロンの重みづけをどのように行えばいいかをいわば自動化するためにあるといえる。\n",
    "この章ではニューラルネットワークをどのように作るかを見ていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63cca6d",
   "metadata": {},
   "source": [
    "さて、ニューラルネットワークは入力層、中間層、出力層の層からなる。  \n",
    "ニューロンのつながり方はパーセプトロンと何ら変わりなくここでは一度パーセプトロンを例に出して考えてみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5fb72",
   "metadata": {},
   "source": [
    "もともとの関数系を思い出す。\n",
    ">$y=0 \\ (b+w_1x_1+w_2x_2\\leq 0)$  \n",
    ">$y=1 \\ (b+w_1x_1+w_2x_2> 0)$ \n",
    " \n",
    "のように入力と重みとバイアスの線形結合で入力の値を決めていた。\n",
    "明示的にバイアスも1という入力に対してのbの重みづけされたものという視点を持てばよりネットワークの形は明瞭になる。  \n",
    ">$y=0 \\ (b\\cdot 1 +w_1x_1+w_2x_2\\leq 0)$  \n",
    ">$y=1 \\ (b\\cdot 1+w_1x_1+w_2x_2> 0)$  \n",
    "\n",
    "といった具合である。ここで以下のような$h(x)$という関数を考える。  \n",
    ">$h(x)=0\\ (x\\leq 0)$  \n",
    ">$h(x)=1\\ (x>0)$  \n",
    "\n",
    "そうすると今までのものはすべて$a=b+w_1x_1+w_2x_2$として\n",
    ">$y=h(a)$\n",
    "\n",
    "と表せる。つまり$a$というノードが**活性化関数**$h()$を通して$y$というノードに変換されることになる。\n",
    "\n",
    "以下ではstep関数の実装を行っている。ステップ関数は今扱った関数と同等の働きをする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827df04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([-1,0.5,1.2])\n",
    "\n",
    "def step_function(x):\n",
    "    y=x>0\n",
    "    return y.astype(int)\n",
    "\n",
    "print(step_function(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8139e",
   "metadata": {},
   "source": [
    "活性化関数にはステップ関数だけでなくsigmoid関数などが存在する。以下に示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c8de16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26894142 0.62245933 0.76852478]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([-1,0.5,1.2])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "print(sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cadf76d",
   "metadata": {},
   "source": [
    ">ステップ関数もシグモイド関数も非線形関数であるが\n",
    "これは$h(x)$が線形すなわち$h(x)=ax+b$のようにかけるときには多層を重ね合わせても結局因子には定数と$x$の1乗項しか出てこないためである。\n",
    "つまり操作h(h(...))は意味をなさないのである。このため非線形性を求められる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2967e",
   "metadata": {},
   "source": [
    "さてここで3層のニューラルネットワークを実装してみる。\n",
    "ここで添え字の確認をしておく、$n$層目の$m$個目のニューロンを$a^{(n)}_m$と書く、\n",
    "また第$n$層目の重みのうち前層$m$番目から次の層$l$番目につなぐものを$w_{lm}^{(n)}$と書く。\n",
    "\n",
    "これは例えば$a^{(1)}_1=w_{11}^{(1)}x_1+w_{12}^{(1)}x_2+b^{(1)}_1$  \n",
    "といった風に書ける。行列表示すれば(あえて太字で書くと)  \n",
    "$\\bm{A}^{(1)}=\\bm{X}\\bm{W}^{(1)}+\\bm{B}^{(1)}$  \n",
    "となる。ただし成分は$\\bm{A}^{(1)}=\\begin{pmatrix}\n",
    "a^{(1)}_1 & a^{(1)}_2 & a^{(1)}_3\n",
    "\\end{pmatrix}$,\n",
    "$\\bm{X}=\\begin{pmatrix}\n",
    "x_1 & x_2\n",
    "\\end{pmatrix}$\n",
    "$\\bm{W}^{(1)}=\\begin{pmatrix}\n",
    "w^{(1)}_{11} & w^{(1)}_{21} & w^{(1)}_{31} \\\\\n",
    "w^{(1)}_{12} & w^{(1)}_{22} & w^{(1)}_{32} \\\\\n",
    "\\end{pmatrix}$,\n",
    "$\\bm{A}^{(1)}=\\begin{pmatrix}\n",
    "a^{(1)}_1 & a^{(1)}_2 & a^{(1)}_3\n",
    "\\end{pmatrix}$  \n",
    "ここで実装を行います。各数値は適当に決めます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf50678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2,)\n",
      "(3,)\n",
      "A1\n",
      "[0.21 0.52 0.83]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([1.0,0.5])\n",
    "W1=np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "B1=np.array([0.01,0.02,0.03])\n",
    "print(W1.shape)\n",
    "print(X.shape)\n",
    "print(B1.shape)\n",
    "\n",
    "A1=np.dot(X,W1)+B1\n",
    "print(\"A1\")\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0097c",
   "metadata": {},
   "source": [
    "活性化関数としてsigmoidを導入すれば"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af0a09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "A1=\n",
      "[0.21 0.52 0.83]\n",
      "Z1=\n",
      "[0.55230791 0.62714777 0.69635493]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([1.0,0.5])\n",
    "W1=np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "B1=np.array([0.01,0.02,0.03])\n",
    "print(X.shape)\n",
    "print(W1.shape)\n",
    "print(B1.shape)\n",
    "\n",
    "A1=np.dot(X,W1)+B1\n",
    "print(\"A1=\")\n",
    "print(A1)\n",
    "\n",
    "\n",
    "Z1=sigmoid(A1)\n",
    "print(\"Z1=\")\n",
    "print(Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4c6be",
   "metadata": {},
   "source": [
    "2層目3層目も同様に行う。ただし3層目では活性化関数はただの恒等関数$\\sigma$を用いてそのままの値を返していることに注意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57ae1ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 shape=(3,)\n",
      "W2 shape=(3, 2)\n",
      "B2 shape=(2,)\n",
      "Z2=[0.62000438 0.7599326 ]\n",
      "Y=[0.31398696 0.68997435]\n"
     ]
    }
   ],
   "source": [
    "#2nd layer\n",
    "W2=np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "B2=np.array([0.1,0.2])\n",
    "\n",
    "print(f\"Z1 shape={Z1.shape}\")\n",
    "print(f\"W2 shape={W2.shape}\")\n",
    "print(f\"B2 shape={B2.shape}\")\n",
    "\n",
    "A2=np.dot(Z1,W2)+B2\n",
    "Z2=sigmoid(A2)\n",
    "print(f\"Z2={Z2}\")\n",
    "\n",
    "\n",
    "#3rd layer\n",
    "def identify_function(x): #sigma\n",
    "    return x\n",
    "\n",
    "W3=np.array([[0.1,0.3],[0.2,0.4]])\n",
    "B3=np.array([0.1,0.2])\n",
    "\n",
    "A3=np.dot(Z2,W3)+B3\n",
    "Y=identify_function(A3)\n",
    "\n",
    "print(f\"Y={Y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f7fec",
   "metadata": {},
   "source": [
    "これで順伝播方向のニューラルネットワークの処理の手続きは完了となる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning (O'Reilly)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
